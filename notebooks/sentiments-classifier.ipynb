{"cells":[{"metadata":{},"cell_type":"markdown","source":"# loading libraries \n"},{"metadata":{},"cell_type":"markdown","source":"importing all necessary libs and packages for the prediction , in this work I will create embeddings features from the text using A Bert model , and uses them as predictors to classify sentiments wich are our labels , this work is a luti classes classification since we have three labels "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport warnings\nwarnings.simplefilter('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import backend as K\nfrom sklearn.metrics import classification_report \nimport os \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import svm\nimport pickle \nimport random ","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fixing the seed to avoid different results in the training (in case of rerun )"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED=42","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\nseed_everything(SEED)    ","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Downoald a bert necessary library for text tokenzation "},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/kpe/bert-for-tf2/master/bert/tokenization/bert_tokenization.py -O tokenization.py\n","execution_count":4,"outputs":[{"output_type":"stream","text":"--2021-03-31 11:12:50--  https://raw.githubusercontent.com/kpe/bert-for-tf2/master/bert/tokenization/bert_tokenization.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13552 (13K) [text/plain]\nSaving to: ‘tokenization.py’\n\ntokenization.py     100%[===================>]  13.23K  --.-KB/s    in 0.001s  \n\n2021-03-31 11:12:51 (18.3 MB/s) - ‘tokenization.py’ saved [13552/13552]\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# librairies for loading BERT MODEL With only URL LINK "},{"metadata":{"trusted":true},"cell_type":"code","source":"import tokenization\nimport tensorflow_hub as hub","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOAD TRAING DATA  AND BUILDING THE MODEL "},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/atrdius/training.tsv',sep='\\t')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"      label                                           sentence\n0  positive  In December alone , the members of the Lithuan...\n1   neutral  Exel wants to serve its industrial customers w...\n2   neutral  Panostaja , headquartered in Tampere , Finland...\n3   neutral  The reductions will be implemented mainly in t...\n4   neutral  `` Because we 're a pension insurance company ...\n5  positive  `` I am very pleased and proud of our performa...\n6  negative  All the ferries had run into trouble just outs...\n7  positive  `` The Government has had a very professional ...\n8   neutral  The company also said that its board of direct...\n9  positive  Kesko 's car import and retailing business , V...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>In December alone , the members of the Lithuan...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>Exel wants to serve its industrial customers w...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>neutral</td>\n      <td>Panostaja , headquartered in Tampere , Finland...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neutral</td>\n      <td>The reductions will be implemented mainly in t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neutral</td>\n      <td>`` Because we 're a pension insurance company ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>positive</td>\n      <td>`` I am very pleased and proud of our performa...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>negative</td>\n      <td>All the ferries had run into trouble just outs...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>positive</td>\n      <td>`` The Government has had a very professional ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>neutral</td>\n      <td>The company also said that its board of direct...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>positive</td>\n      <td>Kesko 's car import and retailing business , V...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\ntrain['target'] = encoder.fit_transform(train['label'])","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the function below is used to convert text (our input ) to  BERT ENCODER  inputs "},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len=100\nnum_classes=3\n","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load bert pretrained model , from tensor flow hub "},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable = True)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = tokenization.FullTokenizer(vocab_file = vocab_file, do_lower_case = do_lower_case)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pooled_output,sequence_output=bert_layer([input_word_ids, input_mask, segment_ids])\n\nclf_output = sequence_output[:, 0, :]\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x = tf.keras.layers.Dropout(rate=0.1)(pooled_output)\n#output = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(x)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = [clf_output])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_train= bert_encode(train['sentence'], tokenizer, max_len = max_len)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_val= bert_encode(x_val, tokenizer, max_len = max_len)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_train = tf.keras.utils.to_categorical(y_train.values)\n#y_val = tf.keras.utils.to_categorical(y_val.values)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VERBOSE = 1\nLR = 0.00001\nEPOCHS =60\nBATCH_SIZE=32\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n                  metrics = [tf.keras.metrics.Accuracy()])","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# create embedinng "},{"metadata":{},"cell_type":"markdown","source":"# As we will use BERT only for features generation , we cut the model at the last encoder layer , and we use the output from there "},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = model.predict(text_train)","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"splitting data for training "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(embeddings, train['target'], shuffle = True, random_state = SEED, test_size = 0.1)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings.shape","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"(2951, 1024)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# classification part :"},{"metadata":{},"cell_type":"markdown","source":"In this part we will use the training data passed throught the model to train a classifier "},{"metadata":{},"cell_type":"markdown","source":"# classifier 1:LGBM "},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def naming(x):\n    return 'col_'+ str(x)\n\ncolumns= list(map(naming ,range(embeddings.shape[1])))","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.DataFrame(x_train,columns=columns)\n\nval_df=pd.DataFrame(x_val,columns=columns)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"(2655, 1024)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df.shape","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"(296, 1024)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.values\ny_val = y_val.values","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(train_df, y_train)\nlgb_val = lgb.Dataset(val_df, y_val)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        \n    \n    \n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'num_class': num_classes,\n    'metric': ['multi_error'],\n    \"learning_rate\": 0.2,\n     \"num_leaves\": 60,\n     \"max_depth\": 10,\n     \"feature_fraction\": 0.45,\n     \"bagging_fraction\": 0.3,\n     \"reg_alpha\": 0.15,\n     \"reg_lambda\": 0.15,\n#      \"min_split_gain\": 0,\n      \"min_child_weight\": 0\n    }\n\nlgbm_model = lgb.train(params, train_set = lgb_train, valid_sets = lgb_val, verbose_eval=5)\n\n\n\n","execution_count":32,"outputs":[{"output_type":"stream","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030094 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 2655, number of used features: 1024\n[LightGBM] [Info] Start training from score -2.112759\n[LightGBM] [Info] Start training from score -0.511454\n[LightGBM] [Info] Start training from score -1.274851\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[5]\tvalid_0's multi_error: 0.243243\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[10]\tvalid_0's multi_error: 0.216216\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[15]\tvalid_0's multi_error: 0.199324\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[20]\tvalid_0's multi_error: 0.199324\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[25]\tvalid_0's multi_error: 0.189189\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[30]\tvalid_0's multi_error: 0.185811\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[35]\tvalid_0's multi_error: 0.185811\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[40]\tvalid_0's multi_error: 0.185811\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[45]\tvalid_0's multi_error: 0.179054\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[50]\tvalid_0's multi_error: 0.172297\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[55]\tvalid_0's multi_error: 0.175676\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","name":"stdout"},{"output_type":"stream","text":"[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[60]\tvalid_0's multi_error: 0.172297\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[65]\tvalid_0's multi_error: 0.168919\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[70]\tvalid_0's multi_error: 0.179054\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[75]\tvalid_0's multi_error: 0.179054\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[80]\tvalid_0's multi_error: 0.172297\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[85]\tvalid_0's multi_error: 0.172297\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[90]\tvalid_0's multi_error: 0.168919\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[95]\tvalid_0's multi_error: 0.168919\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","name":"stdout"},{"output_type":"stream","text":"[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\tvalid_0's multi_error: 0.168919\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# TESTING :\nin this part I test the model using the test data , after preapring it "},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('../input/atrdius/testing.tsv',sep='\\t')\ntext_test= bert_encode(test['sentence'], tokenizer, max_len = max_len)\ntest_embeddings = model.predict(text_test)\ntest_df=pd.DataFrame(test_embeddings,columns=columns)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\ntest['target'] = encoder.fit_transform(test['label'])","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_predict=lgbm_model.predict(test_df ) \nlabel_predict=encoder.classes_[np.argmax(Y_predict,axis=1)] \nprint(classification_report(test['label'],label_predict,digits=4))","execution_count":35,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    negative     0.7642    0.6104    0.6787       154\n     neutral     0.8060    0.9501    0.8721       761\n    positive     0.8211    0.5755    0.6767       351\n\n    accuracy                         0.8049      1266\n   macro avg     0.7971    0.7120    0.7425      1266\nweighted avg     0.8051    0.8049    0.7944      1266\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# SVM CLASSIFIER "},{"metadata":{},"cell_type":"markdown","source":"Data standarization with builtin sckit learn function Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscaler = StandardScaler()\nscaler.fit(embeddings)\nembeddings_scaled=scaler.transform(embeddings)\ntest_scaled=scaler.transform(test_embeddings)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nSVM = svm.SVC(C=0.5, kernel='sigmoid', degree=3, gamma='auto',probability=True)\nSVM.fit(embeddings_scaled,train['target'].values)\n# predict the labels on testing data \npredictions_SVM = SVM.predict(test_scaled)\nsvm_label_predict=encoder.classes_[predictions_SVM]\n\n# Use accuracy_score function to get the accuracy\nprint(classification_report(test['label'],svm_label_predict,digits=4))\n\n","execution_count":37,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    negative     0.7329    0.6948    0.7133       154\n     neutral     0.8268    0.9409    0.8801       761\n    positive     0.8386    0.6068    0.7041       351\n\n    accuracy                         0.8183      1266\n   macro avg     0.7994    0.7475    0.7659      1266\nweighted avg     0.8186    0.8183    0.8111      1266\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# blend of SVM and LGBM  for prediction "},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_predict_proba=SVM.predict_proba(test_scaled)\nGlobal_predict=(0.4*Y_predict+0.6*svm_predict_proba)\nGlobal_label_predict=encoder.classes_[np.argmax(Global_predict,axis=1)] \nprint(classification_report(test['label'],Global_label_predict,digits=4))","execution_count":45,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n    negative     0.7519    0.6494    0.6969       154\n     neutral     0.8205    0.9488    0.8800       761\n    positive     0.8419    0.6068    0.7053       351\n\n    accuracy                         0.8175      1266\n   macro avg     0.8047    0.7350    0.7607      1266\nweighted avg     0.8181    0.8175    0.8093      1266\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# saving models that will be used in the api "},{"metadata":{},"cell_type":"markdown","source":"saving the lgbm model"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_model.save_model(f'lgbm_model.lgb')","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"<lightgbm.basic.Booster at 0x7ff565016190>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"saving scaler "},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('scaler.pickle', 'wb') as f:\n    pickle.dump(scaler, f)","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"saving SVM as pickle "},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('SVM.pickle', 'wb') as f:\n    pickle.dump(SVM, f)","execution_count":41,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}